{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f62e5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ctypes\n",
    "import shutil\n",
    "import os\n",
    "MessageBox = ctypes.windll.user32.MessageBoxW\n",
    "from System_Assessment_Generate_Models_Variables import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a54162bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql_to_df(sql,model):\n",
    "    con = sqlite3.connect(model)\n",
    "#     print(sql)\n",
    "    df = pd.read_sql(sql, con)\n",
    "    con.close()\n",
    "    return df\n",
    "\n",
    "def execute_sql(sqls,model):\n",
    "    con = sqlite3.connect(model)\n",
    "    cur = con.cursor()\n",
    "    if type(sqls) == list:\n",
    "        for sql in sqls:\n",
    "#             print(sql)\n",
    "            cur.execute(sql)\n",
    "    else:\n",
    "        sql = sqls\n",
    "#         print(sql)\n",
    "        cur.execute(sql)\n",
    "    cur.close()\n",
    "    con.commit()\n",
    "    con.close()\n",
    "  \n",
    "def df_to_sql(df,table_name,model):\n",
    "    conn = sqlite3.connect(model)\n",
    "    df.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "def generate_script(script_path,mu_path):  \n",
    "    script_path_new = os.path.splitext(mu_path)[0] + '.cs'\n",
    "    shutil.copy(script_path, script_path_new)\n",
    "    \n",
    "def generate_mupp(model_original,mu_path):\n",
    "    mupp_path_original = os.path.splitext(model_original)[0] + '.mupp'\n",
    "    mupp_path_new = os.path.splitext(mu_path)[0] + '.mupp'   \n",
    "    with open(mupp_path_original, 'r') as source_file, open(mupp_path_new, 'w') as output_file:\n",
    "        for line in source_file:\n",
    "            if line.startswith(r'   DBFilePath = |.'):\n",
    "                output_file.write('   DBFilePath = |.\\\\' + os.path.splitext(os.path.basename(mu_path))[0] + '.sqlite|\\n')\n",
    "            else:\n",
    "                output_file.write(line)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "197ec907",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Early error catching\n",
    "if generate_bsf:\n",
    "\n",
    "    for gwi in gwis:\n",
    "        if np.mod(gwi,11.2) > 0:\n",
    "\n",
    "            message = 'GWI ' + str(gwi) + ' is not a multiple of 11.2\\n\\nContinue?'\n",
    "\n",
    "            if MessageBox(None, message, 'Warning', 4) == 7:\n",
    "                MessageBox(None, \"Please correct gwis\", 'Info', 0)\n",
    "                raise ValueError(message)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "613143a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_future:\n",
    "    \n",
    "    mu_paths = []\n",
    "\n",
    "    keep_cols = ['Catchment','Zone','Year','Pop_ResLD','Pop_ResHD','Pop_Mixed','Area_Com','Area_Ind','Area_Inst']\n",
    "    pop_df = pd.read_excel(population_sheet,sheet_name=population_tab,usecols=keep_cols,dtype={'Catchment': str})\n",
    "\n",
    "    # pop_df = pop_df[pop_df.Catchment!='84394S']\n",
    "    # pop_df = pop_df[pop_df.Catchment!='84279']\n",
    "    # pop_df = pop_df.drop(pop_df.index[-1])\n",
    "\n",
    "    # Melt the DataFrame to combine the columns\n",
    "    columns_to_combine = ['Pop_ResLD', 'Pop_ResHD', 'Pop_Mixed', 'Area_Com', 'Area_Ind', 'Area_Inst']\n",
    "    pop_df_melt = pd.melt(pop_df, id_vars=['Catchment', 'Zone', 'Year'], value_vars=columns_to_combine, var_name='Type', value_name='Value')\n",
    "    pop_df_melt[['Major_type', 'Minor_type']] = pop_df_melt['Type'].str.split('_', n=1, expand=True)\n",
    "    pop_df_melt = pop_df_melt.drop(columns=['Type'])\n",
    "    pop_df_melt['MUID'] = pop_df_melt.Catchment + '_' + pop_df_melt.Minor_type\n",
    "    pop_df_melt\n",
    "\n",
    "    #Check for missing catchments\n",
    "    catchment_years = []\n",
    "    sql = \"SELECT catchmentid FROM msm_Loadpoint GROUP BY catchmentid\"\n",
    "    muids = list(sql_to_df(sql,model_original).catchmentid)\n",
    "\n",
    "    for muid in muids:\n",
    "        for year_scenario in year_scenario_list:\n",
    "            year = year_scenario[0]\n",
    "            catchment_years.append([muid,year])\n",
    "    catchment_year_df = pd.DataFrame(catchment_years,columns=(['Catchment','Year']))\n",
    "    catchment_year_df \n",
    "\n",
    "    merged = catchment_year_df.merge(pop_df[['Catchment', 'Year']], on=['Catchment', 'Year'], how='left', indicator=True)\n",
    "\n",
    "    not_founds = merged[merged['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "\n",
    "    if len(not_founds) > 0:\n",
    "        message = \"WARNING.The following catchment/year combinations are not found\\n\\n\"\n",
    "        for index, row in not_founds.iterrows():\n",
    "            message += row[0] + ', ' + str(row[1]) + '.\\n'\n",
    "        message += '\\nContinue?'\n",
    "\n",
    "        if MessageBox(None, message, 'Warning', 4) == 7:\n",
    "            MessageBox(None, \"Please report the missing catchment(s)\", 'Info', 0)\n",
    "            raise ValueError(message)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    #Generate runoff hotstart model without C#\n",
    "    model_name = model_area + '_Runoff_Hotstart.sqlite'\n",
    "    mu_path = output_folder + \"\\\\\" + model_name    \n",
    "    os.remove(mu_path) if os.path.exists(mu_path) else None\n",
    "    shutil.copyfile(model_original, mu_path)\n",
    "    generate_mupp(model_original,mu_path)\n",
    "\n",
    "    for year_scenario in year_scenario_list:\n",
    "        year = year_scenario[0]\n",
    "        scenario = year_scenario[1]\n",
    "        turnons = year_scenario[2]\n",
    "\n",
    "        model_name = model_area + '_' + str(year) + 'pop_V' + str(version) + '.sqlite'\n",
    "\n",
    "        if os.path.basename(model_original) == model_name:\n",
    "\n",
    "            message = \"Tool ends. For year \" + str(year) + \", the new model name '\" + model_name + \"' is the same as the original.\"\n",
    "            MessageBox(None, message, 'Info', 0)\n",
    "            raise ValueError(\"message\")\n",
    "\n",
    "        #Delete sqlite, mupp and cs if they exist and create new\n",
    "        mu_path = output_folder + \"\\\\\" + model_name\n",
    "        \n",
    "        mu_paths.append(mu_path)\n",
    "\n",
    "        generate_script(script_path,mu_path)\n",
    "        generate_mupp(model_original,mu_path)\n",
    "\n",
    "        os.remove(mu_path) if os.path.exists(mu_path) else None\n",
    "        shutil.copyfile(model_original, mu_path)\n",
    "\n",
    "        sql = \"SELECT MUID FROM msm_Project WHERE enable_hd = 1\"\n",
    "        muids = list(sql_to_df(sql,mu_path).muid)\n",
    "        for muid in muids:\n",
    "            muid_new = muid.replace(str(year_original) + 'p',str(year) + 'p')\n",
    "            sql = \"UPDATE msm_Project SET muid = '\" + muid_new + \"' WHERE muid = '\" + muid + \"'\"\n",
    "            execute_sql(sql, mu_path)\n",
    "\n",
    "            sql = \"UPDATE msm_ProjectOutput SET simulationid = '\" + muid_new + \"' WHERE simulationid = '\" + muid + \"'\"\n",
    "            execute_sql(sql, mu_path)\n",
    "\n",
    "        sql = \"UPDATE msm_Project SET IncludeToBatchNo = 0 WHERE MUID LIKE '%h-AES_%'\"\n",
    "        execute_sql(sql, mu_path)\n",
    "        \n",
    "        sql = \"UPDATE msm_Project SET scenarioname = '\" + scenario + \"' WHERE MUID NOT LIKE '%h-AES_%' AND enable_hd = 1\"\n",
    "        execute_sql(sql, mu_path)\n",
    "\n",
    "        for turnon in turnons:\n",
    "            sql = \"UPDATE msm_Project SET IncludeToBatchNo = 1 WHERE MUID LIKE '%h-AES_%' AND MUID LIKE '%\" + turnon + \"%' \"\n",
    "            sql += \"AND SUBSTR(scenarioname,1,\" + str(len(scenario)) + \") = '\" + scenario  + \"'\"\n",
    "            execute_sql(sql, mu_path)\n",
    "\n",
    "        pop_df_melt_year = pop_df_melt[pop_df_melt.Year==year]\n",
    "\n",
    "        df_to_sql(pop_df_melt_year,'New_Population',mu_path)\n",
    "\n",
    "        sql = \"UPDATE msm_Loadpoint SET Population = \"\n",
    "        sql += \"(SELECT Value FROM New_Population WHERE MUID = msm_Loadpoint.muid AND Major_Type = 'Pop')\"\n",
    "        execute_sql(sql, mu_path)\n",
    "\n",
    "        sql = \"UPDATE msm_Loadpoint SET ICIArea = \"\n",
    "        sql += \"(SELECT Value FROM New_Population WHERE MUID = msm_Loadpoint.muid AND Major_Type = 'Area')\"\n",
    "        execute_sql(sql, mu_path)\n",
    "\n",
    "        sql = \"UPDATE msm_Loadpoint SET loadflow = PerCapitaLoad * Population / 86400 WHERE LoadCategory = 'Mixed' OR LoadCategory = 'ResLD' OR LoadCategory = 'ResHD'\"\n",
    "        execute_sql(sql, mu_path)\n",
    "        sql = \"UPDATE msm_Loadpoint SET loadflow = PerAreaLoad * ICIArea / 86400 WHERE LoadCategory = 'Commercial' OR LoadCategory = 'Industrial' OR LoadCategory = 'Institutional'\"\n",
    "        execute_sql(sql, mu_path)\n",
    "\n",
    "        sql = \"DROP TABLE New_Population\"\n",
    "        execute_sql(sql, mu_path)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8481714c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate BSF models\n",
    "if generate_bsf:\n",
    "\n",
    "    if not generate_future:\n",
    "        mu_paths = []\n",
    "        for f in os.listdir(output_folder):\n",
    "            if f[-7:]==\".sqlite\" and not 'BSF' in f and not 'xADWF' in f:\n",
    "                mu_paths.append(output_folder + '\\\\' + f)\n",
    "\n",
    "\n",
    "    for mu_path_original in mu_paths:\n",
    "                \n",
    "        sqls = []\n",
    "\n",
    "        pop_pos = mu_path_original.lower().find('pop')\n",
    "        year = mu_path_original[pop_pos-4:pop_pos]\n",
    "\n",
    "        for gwi in gwis:\n",
    "\n",
    "            gwi_str = str(gwi).replace('.','p')\n",
    "\n",
    "            mu_path = mu_path_original[:-7]  + \"_BSF_\" + gwi_str + \"k.sqlite\"\n",
    "            os.remove(mu_path) if os.path.exists(mu_path) else None\n",
    "            shutil.copyfile(mu_path_original, mu_path)\n",
    "            \n",
    "            print('Generating ' + mu_path)\n",
    "\n",
    "            generate_script(script_path,mu_path)\n",
    "            generate_mupp(mu_path_original,mu_path)\n",
    "\n",
    "            sql = \"SELECT muid, area, nettypeno FROM msm_Catchment WHERE nettypeno <> 2\"\n",
    "            catchments = sql_to_df(sql,mu_path)\n",
    "\n",
    "            sqls = []\n",
    "            for index, row in catchments.iterrows():\n",
    "                MUID = str(row[0])\n",
    "                Area = row[1]\n",
    "                NetTypeNo = str(row[2])\n",
    "\n",
    "                flow = Area * gwi\n",
    "\n",
    "                sql = \"UPDATE msm_LoadPoint SET MUID = '\" + MUID + \"_BSF', loadflow = \" + str(flow / 86400 / 10000) + \", Description = 'BSF', LoadCategoryNo = 100, LoadCategory = 'BSF', LoadSubCategory = 'BSF_' & LoadLocation \"\n",
    "                sql += \"WHERE MUID = '\" + MUID + \"_Load_8'\"\n",
    "\n",
    "                sqls.append(sql)\n",
    "\n",
    "            execute_sql(sqls, mu_path)\n",
    "\n",
    "            muid_new = model_area + \"_BSF_\" + gwi_str + \"k_\" + year + \"pop_\"\n",
    "            \n",
    "            sql =  \"DELETE FROM msm_Project WHERE enable_catchment = 1\"\n",
    "            execute_sql(sql, mu_path)\n",
    "\n",
    "            sql = \"SELECT muid FROM msm_Project\"\n",
    "            sims = sql_to_df(sql,mu_path)\n",
    "            \n",
    "            for index, row in sims.iterrows():\n",
    "                muid = row[0]\n",
    "                if index == 0:\n",
    "                    sql = \"UPDATE msm_Project SET MUID = '\" + muid_new + \"', ScenarioName = 'Base', Description = 'BSF' WHERE MUID = '\" + muid + \"'\"\n",
    "                    execute_sql(sql, mu_path)\n",
    "                    sql = \"UPDATE msm_ProjectOutput SET simulationid = '\" + muid_new + \"' WHERE simulationid = '\" + muid + \"'\"          \n",
    "                    execute_sql(sql, mu_path)\n",
    "                else:\n",
    "                    sql =  \"DELETE FROM msm_Project WHERE simulationid = '\" + muid + \"'\"\n",
    "                    execute_sql(sql, mu_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a20c003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating J:\\SEWER_AREA_MODELS\\FSA\\03_SIMULATION_WORK\\System_Assessment\\Model_Generation\\FSA_2030pop_V104_3xADWF.sqlite\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "if generate_xadwf:\n",
    "    deficit_list = []\n",
    "    diurnals = []\n",
    "\n",
    "    if not generate_future:\n",
    "        mu_paths = []\n",
    "        for f in os.listdir(output_folder):\n",
    "            if f[-7:]==\".sqlite\" and not 'BSF' in f and not 'xADWF' in f:\n",
    "                mu_paths.append(output_folder + '\\\\' + f)\n",
    "\n",
    "    for i, mu_path_original in enumerate(mu_paths):\n",
    "                \n",
    "        sqls = []\n",
    "\n",
    "        pop_pos = mu_path_original.lower().find('pop')\n",
    "        year = mu_path_original[pop_pos-4:pop_pos]\n",
    "\n",
    "        sql = \"SELECT Sum(loadflow) AS WaterLoad FROM msm_loadpoint WHERE loadcategory = 'Baseflow'\"\n",
    "        gwi_global = sql_to_df(sql,mu_path_original).iloc[0,0]\n",
    "#         gwi_global = list(sql_to_df(sql,mu_path_original).WaterLoad)[0]\n",
    "        \n",
    "\n",
    "        sql = \"SELECT ms_DPProfileD.ScheduleID AS Day_Type, ms_DPPatternD.Sqn AS [Hour], Sum(msm_Loadpoint.loadflow*ms_DPPatternD.DPValue) + \" + str(gwi_global) + \" AS Discharge \"\n",
    "        sql += \"FROM ((msm_Loadpoint INNER JOIN msm_BBoundary ON msm_Loadpoint.LoadCategoryNo = msm_BBoundary.LoadCategoryNo) INNER JOIN ms_DPProfileD ON msm_BBoundary.DPProfileID = ms_DPProfileD.ProfileID) INNER JOIN ms_DPPatternD ON ms_DPProfileD.PatternID = ms_DPPatternD.PatternID \"\n",
    "        sql += \"WHERE msm_Loadpoint.Active = 1 AND ms_DPProfileD.Active = 1 AND ms_DPPatternD.Active = 1 AND msm_BBoundary.Active = 1 \"\n",
    "        sql += \"GROUP BY ms_DPProfileD.ScheduleID, ms_DPPatternD.Time \"\n",
    "        sql += \"HAVING (LOWER(SUBSTR(ms_DPProfileD.ScheduleID,1,7))='weekday' Or LOWER(SUBSTR(ms_DPProfileD.ScheduleID,1,7))='weekend') AND ms_DPPatternD.Sqn <> 0 \"\n",
    "        sql += \"ORDER BY scheduleid, time\"\n",
    "                        \n",
    "        diurnal_wws = sql_to_df(sql,mu_path_original)        \n",
    "        diurnal_wws.Hour = diurnal_wws.index\n",
    "        diurnal_wws.loc[diurnal_wws.index > 23, 'HOUR'] = diurnal_wws.index[diurnal_wws.index > 23] - 24\n",
    "\n",
    "        for times_adwf in times_adwf_list:\n",
    "            \n",
    "            mu_path = mu_path_original[:-7]  + \"_\" + str(times_adwf).replace('.','p') + \"xADWF.sqlite\"\n",
    "            os.remove(mu_path) if os.path.exists(mu_path) else None\n",
    "            shutil.copyfile(mu_path_original, mu_path)\n",
    "            \n",
    "            print('Generating ' + mu_path)\n",
    "\n",
    "            generate_script(script_path,mu_path)\n",
    "            generate_mupp(mu_path_original,mu_path)\n",
    "            \n",
    "            sqls = []\n",
    "\n",
    "            sql = \"SELECT muid FROM msm_Catchment\"\n",
    "            muids = list(sql_to_df(sql,mu_path).muid)\n",
    "            for muid in muids:\n",
    "\n",
    "                sql = \"SELECT SUM(loadflow) FROM msm_Loadpoint WHERE CatchmentID = '\" + muid + \"' AND msm_Loadpoint.Active = 1\"\n",
    "                adwf = sql_to_df(sql,mu_path).iloc[0,0]\n",
    "\n",
    "                sql = \"SELECT SUM(loadflow) FROM msm_Loadpoint WHERE CatchmentID = '\" + muid + \"' AND LoadCategory = 'Baseflow' AND msm_Loadpoint.Active = 1\"\n",
    "                gwi = sql_to_df(sql,mu_path).iloc[0,0]\n",
    "\n",
    "                sql = \"SELECT Max(SumOfDPValue) \"\n",
    "                sql += \"FROM (SELECT \"\n",
    "                sql += \"ms_DPProfileD.ProfileID, Sum(msm_Loadpoint.loadflow*ms_DPPatternD.DPValue) AS SumOfDPValue \"\n",
    "                sql += \"FROM ((msm_Loadpoint INNER JOIN msm_BBoundary ON msm_Loadpoint.LoadCategoryNo = msm_BBoundary.LoadCategoryNo) INNER JOIN ms_DPProfileD ON msm_BBoundary.DPProfileID = ms_DPProfileD.ProfileID) INNER JOIN ms_DPPatternD ON ms_DPProfileD.PatternID = ms_DPPatternD.PatternID \"\n",
    "                sql += \"WHERE msm_Loadpoint.Active = 1 And msm_BBoundary.Active = 1 And ms_DPProfileD.Active = 1 And ms_DPPatternD.Active = 1 \"\n",
    "                sql += \"GROUP BY ms_DPProfileD.ProfileID,ms_DPPatternD.MUID, msm_Loadpoint.CatchmentID, ms_DPProfileD.ScheduleID \"\n",
    "                sql += \"HAVING msm_Loadpoint.CatchmentID = '\" + muid + \"' AND (LOWER(SUBSTR(ms_DPProfileD.ScheduleID,1,7))='weekday' Or LOWER(SUBSTR(ms_DPProfileD.ScheduleID,1,7))='weekend'))\"\n",
    "                \n",
    "                pww = sql_to_df(sql,mu_path).iloc[0,0]\n",
    "                pdwf = gwi + pww\n",
    "                deficit = times_adwf * adwf - pdwf\n",
    "\n",
    "                deficit_list.append([os.path.basename(mu_path),muid,deficit])\n",
    "\n",
    "                sql = \"UPDATE msm_Loadpoint SET muid = '\" + muid + \"_\" + str(times_adwf).replace('.','p') + \"xADWF', \"\n",
    "                sql += \"loadflow = \" + str(deficit) + \", Description = '\" + str(times_adwf).replace('.','p') + \"xADWF', \"\n",
    "                sql += \"LoadCategoryNo = 1, LoadCategory = 'X-ADWF', LoadSubCategory = 'X-ADWF_' & LoadLocation \"\n",
    "                sql += \"WHERE muid = '\" + muid + \"_Load_8' and msm_Loadpoint.Active = 1\"\n",
    "                sqls.append(sql)\n",
    "\n",
    "            execute_sql(sqls, mu_path)\n",
    "            \n",
    "            muid_new = model_area + \"_\" + str(times_adwf).replace('.','p') + \"ADWF_\" + year + \"pop_\"\n",
    "            sql =  \"DELETE FROM msm_Project WHERE enable_catchment = 1\"\n",
    "            execute_sql(sql, mu_path)\n",
    "            sql = \"SELECT muid FROM msm_Project\"\n",
    "            sims = sql_to_df(sql,mu_path)\n",
    "            for index, row in sims.iterrows():\n",
    "                muid = row[0]\n",
    "                if index == 0:\n",
    "                    sql = \"UPDATE msm_Project SET MUID = '\" + muid_new + \"', ScenarioName = 'Base', Description = 'BSF' WHERE MUID = '\" + muid + \"'\"\n",
    "                    execute_sql(sql, mu_path)\n",
    "                    sql = \"UPDATE msm_ProjectOutput SET simulationid = '\" + muid_new + \"' WHERE simulationid = '\" + muid + \"'\"          \n",
    "                    execute_sql(sql, mu_path)\n",
    "                else:\n",
    "                    sql =  \"DELETE FROM msm_Project WHERE simulationid = '\" + muid + \"'\"\n",
    "                    execute_sql(sql, mu_path)\n",
    "\n",
    "            sql = \"SELECT Sum(loadflow) FROM msm_Loadpoint WHERE LoadCategory = 'X-ADWF' AND Active = 1\"\n",
    "            deficit = sql_to_df(sql,mu_path).iloc[0,0]\n",
    "            \n",
    "            diurnal_wws['Model'] = os.path.basename(mu_path)\n",
    "            diurnal_wws['Deficit'] = deficit\n",
    "            diurnal_wws = diurnal_wws[['Model','Day_Type','Hour','Discharge','Deficit']]\n",
    "            \n",
    "            if i == 0:\n",
    "                diurnals_df = diurnal_wws.copy()\n",
    "            else:\n",
    "                diurnals_df = pd.concat([diurnals_df,diurnal_wws])           \n",
    "\n",
    "    deficit_df = pd.DataFrame(deficit_list,columns=['Model','Catchment','Deficit'])\n",
    "    deficit_df.to_csv(output_folder + '\\\\Deficits.csv',index=False)\n",
    "\n",
    "    diurnals_df = pd.DataFrame(diurnals,columns=['Model','Day_Type','Hour','Discharge','Deficit'])\n",
    "    diurnals_df.to_csv(output_folder + '\\\\X-ADWF_Diurnals.csv',index=False)\n",
    "    \n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba8f3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_sealed_vfd:\n",
    "    \n",
    "    if not generate_future:\n",
    "        mu_paths = []\n",
    "        for f in os.listdir(output_folder):\n",
    "            if f[-7:]==\".sqlite\" and not 'VFD' in f:\n",
    "                mu_paths.append(output_folder + '\\\\' + f)\n",
    " \n",
    "    if seal_all and vfd_all:\n",
    "        suffix = \"S_V_\"\n",
    "        file_suffix = \"_Sealed_VFD\"\n",
    "    elif seal_all and not vfd_all:\n",
    "        suffix = \"S_\"\n",
    "        file_suffix = \"_Sealed\"\n",
    "    elif not seal_all and vfd_all:        \n",
    "        suffix = \"V_\"\n",
    "        file_suffix = \"_VFD\" \n",
    "    else:\n",
    "        message = \"Error! generate_sealed_vfd set to True but both vfd_all and seal_all set to False\"\n",
    "        raise ValueError(message)\n",
    "\n",
    "    for mu_path_original in mu_paths:\n",
    "                \n",
    "        sqls = []\n",
    "\n",
    "        mu_path = mu_path_original[:-7] + file_suffix + \".sqlite\"\n",
    "        os.remove(mu_path) if os.path.exists(mu_path) else None\n",
    "        shutil.copyfile(mu_path_original, mu_path)\n",
    "        \n",
    "        generate_script(script_path,mu_path)\n",
    "        generate_mupp(mu_path_original,mu_path)\n",
    "        \n",
    "        print('Generating ' + mu_path)\n",
    "\n",
    "        if vfd_all == True:\n",
    "\n",
    "            pumps_for_VFD = []\n",
    "            pumps_turn_off = []\n",
    "\n",
    "            sqls.append(\"INSERT INTO ms_Tab (muid,altid,active,description,typeno) SELECT 'Generic_Pump_Min',0,1,'Use for PS passing all inflow',2\")\n",
    "            sqls.append(\"INSERT INTO ms_Tab (muid,altid,active,description,typeno) SELECT 'Generic_Pump_Max',0,1,'Use for PS passing all inflow',2\")\n",
    "\n",
    "            sqls.append(\"INSERT INTO ms_TabD (muid,altid,active,tabid,sqn,value1,value2) SELECT 'Generic_Pump_Max-1',0,1,'Generic_Pump_Max',1, 0, 50\")\n",
    "            sqls.append(\"INSERT INTO ms_TabD (muid,altid,active,tabid,sqn,value1,value2) SELECT 'Generic_Pump_Max-2',0,1,'Generic_Pump_Max',2, 100, 50\")\n",
    "            \n",
    "            sqls.append(\"INSERT INTO ms_TabD (muid,altid,active,tabid,sqn,value1,value2) SELECT 'Generic_Pump_Min-1',0,1,'Generic_Pump_Min',1, 0, 0\")\n",
    "            sqls.append(\"INSERT INTO ms_TabD (muid,altid,active,tabid,sqn,value1,value2) SELECT 'Generic_Pump_Min-2',0,1,'Generic_Pump_Min',2, 100, 0\")\n",
    "            \n",
    "            sql = \"SELECT MUID FROM msm_Project WHERE enable_hd = 1\"\n",
    "            muids = list(sql_to_df(sql,mu_path).muid)\n",
    "            for muid in muids:\n",
    "                if len(muid) <= 40 - len(suffix):\n",
    "                    muid_new = muid + suffix\n",
    "                    sqls.append(\"UPDATE msm_Project SET muid = '\" + muid_new + \"' WHERE muid = '\" + muid + \"'\")        \n",
    "                    sqls.append(\"UPDATE msm_ProjectOutput SET simulationid = '\" + muid_new + \"' WHERE simulationid = '\" + muid + \"'\")            \n",
    "                                \n",
    "            sql = \"SELECT assetname, muid, startlevel from msm_Pump ORDER BY assetname, startlevel\"\n",
    "            pumps = sql_to_df(sql,mu_path)\n",
    "\n",
    "            previous_asset = 'xxxx'\n",
    "            for index, row in pumps.iterrows():\n",
    "                asset = str(row[0])\n",
    "                if not asset in excluded_asset_names and asset != 'None':\n",
    "                    muid = str(row[1])\n",
    "                    if previous_asset != asset:\n",
    "                        pumps_for_VFD.append(muid)\n",
    "                    else:\n",
    "                        pumps_turn_off.append (muid)\n",
    "                    previous_asset = asset\n",
    "                else:\n",
    "                    print ('Skipped ' + str(row[1]))\n",
    "\n",
    "            for pump_for_VFD in pumps_for_VFD:\n",
    "                sqls.append(\"UPDATE msm_Pump SET qmaxsetid = 'Generic_Pump_Max',  qminsetid = 'Generic_Pump_Min', speedno = 2, captypeno = 2, controltypeno = 1, startlevel = stoplevel + 0.1,  wetwellsetpoint = stoplevel + 0.1  WHERE muid = '\" + pump_for_VFD + \"'\")\n",
    "                sqls.append(\"UPDATE msm_RTC SET applyno = 0 WHERE pumpid = '\" + pump_for_VFD + \"'\")\n",
    "\n",
    "            for pump_turn_off in pumps_turn_off:\n",
    "                sqls.append(\"UPDATE msm_Pump SET startlevel = startlevel + 100, stoplevel = stoplevel + 100,  controltypeno = 1 WHERE MUID = '\" + pump_turn_off + \"'\")\n",
    "                sqls.append(\"UPDATE msm_RTC SET applyno = 0 WHERE pumpid = '\" + pump_turn_off + \"'\")\n",
    "\n",
    "        execute_sql(sqls, mu_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39_mike",
   "language": "python",
   "name": "py39_mike"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
