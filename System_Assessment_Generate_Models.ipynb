{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f62e5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ctypes\n",
    "import shutil\n",
    "import os\n",
    "MessageBox = ctypes.windll.user32.MessageBoxW\n",
    "from System_Assessment_Generate_Models_Variables import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a54162bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql_to_df(sql,model):\n",
    "    con = sqlite3.connect(model)\n",
    "    df = pd.read_sql(sql, con)\n",
    "    con.close()\n",
    "    return df\n",
    "\n",
    "def execute_sql(sqls,model):\n",
    "    con = sqlite3.connect(model)\n",
    "    cur = con.cursor()\n",
    "    if type(sqls) == list:\n",
    "        for sql in sqls:\n",
    "            print(sql)\n",
    "            cur.execute(sql)\n",
    "    else:\n",
    "        sql = sqls\n",
    "        cur.execute(sql)\n",
    "    cur.close()\n",
    "    con.commit()\n",
    "    con.close()\n",
    "  \n",
    "def df_to_sql(df,table_name,model):\n",
    "    conn = sqlite3.connect(model)\n",
    "    df.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "def generate_script(script_path,mu_path):  \n",
    "    script_path_new = os.path.splitext(mu_path)[0] + '.cs'\n",
    "    shutil.copy(script_path, script_path_new)\n",
    "    \n",
    "def generate_mupp(model_original,mu_path):\n",
    "    mupp_path_original = os.path.splitext(model_original)[0] + '.mupp'\n",
    "    mupp_path_new = os.path.splitext(mu_path)[0] + '.mupp'   \n",
    "    with open(mupp_path_original, 'r') as source_file, open(mupp_path_new, 'w') as output_file:\n",
    "        for line in source_file:\n",
    "            if line.startswith(r'   DBFilePath = |.'):\n",
    "                output_file.write('   DBFilePath = |.\\\\' + os.path.splitext(model_name)[0] + '.sqlite|\\n')\n",
    "            else:\n",
    "                output_file.write(line)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b39e9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea8bd54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "613143a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_future:\n",
    "    \n",
    "    mu_paths = []\n",
    "\n",
    "    keep_cols = ['Catchment','Zone','Year','Pop_ResLD','Pop_ResHD','Pop_Mixed','Area_Com','Area_Ind','Area_Inst']\n",
    "    pop_df = pd.read_excel(population_sheet,sheet_name=population_tab,usecols=keep_cols,dtype={'Catchment': str})\n",
    "\n",
    "    # pop_df = pop_df[pop_df.Catchment!='84394S']\n",
    "    # pop_df = pop_df[pop_df.Catchment!='84279']\n",
    "    # pop_df = pop_df.drop(pop_df.index[-1])\n",
    "\n",
    "    # Melt the DataFrame to combine the columns\n",
    "    columns_to_combine = ['Pop_ResLD', 'Pop_ResHD', 'Pop_Mixed', 'Area_Com', 'Area_Ind', 'Area_Inst']\n",
    "    pop_df_melt = pd.melt(pop_df, id_vars=['Catchment', 'Zone', 'Year'], value_vars=columns_to_combine, var_name='Type', value_name='Value')\n",
    "    pop_df_melt[['Major_type', 'Minor_type']] = pop_df_melt['Type'].str.split('_', n=1, expand=True)\n",
    "    pop_df_melt = pop_df_melt.drop(columns=['Type'])\n",
    "    pop_df_melt['MUID'] = pop_df_melt.Catchment + '_' + pop_df_melt.Minor_type\n",
    "    pop_df_melt\n",
    "\n",
    "    #Check for missing catchments\n",
    "    catchment_years = []\n",
    "    sql = \"SELECT catchmentid FROM msm_Loadpoint GROUP BY catchmentid\"\n",
    "    muids = list(sql_to_df(sql,model_original).catchmentid)\n",
    "\n",
    "    for muid in muids:\n",
    "        for year_scenario in year_scenario_list:\n",
    "            year = year_scenario[0]\n",
    "            catchment_years.append([muid,year])\n",
    "    catchment_year_df = pd.DataFrame(catchment_years,columns=(['Catchment','Year']))\n",
    "    catchment_year_df \n",
    "\n",
    "    merged = catchment_year_df.merge(pop_df[['Catchment', 'Year']], on=['Catchment', 'Year'], how='left', indicator=True)\n",
    "\n",
    "    not_founds = merged[merged['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "\n",
    "    if len(not_founds) > 0:\n",
    "        message = \"WARNING.The following catchment/year combinations are not found\\n\\n\"\n",
    "        for index, row in not_founds.iterrows():\n",
    "            message += row[0] + ', ' + str(row[1]) + '.\\n'\n",
    "        message += '\\nContinue?'\n",
    "\n",
    "        if MessageBox(None, message, 'Warning', 4) == 7:\n",
    "            MessageBox(None, \"Please report the missing catchment(s)\", 'Info', 0)\n",
    "            raise ValueError(message)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    for year_scenario in year_scenario_list:\n",
    "        year = year_scenario[0]\n",
    "        scenario = year_scenario[1]\n",
    "        turnons = year_scenario[2]\n",
    "\n",
    "        model_name = model_area + '_' + str(year) + 'pop_V' + str(version) + '.sqlite'\n",
    "\n",
    "        if os.path.basename(model_original) == model_name:\n",
    "\n",
    "            message = \"Tool ends. For year \" + str(year) + \", the new model name '\" + model_name + \"' is the same as the original.\"\n",
    "            MessageBox(None, message, 'Info', 0)\n",
    "            raise ValueError(\"message\")\n",
    "\n",
    "        #Delete sqlite, mupp and cs if they exist and create new\n",
    "        mu_path = output_folder + \"\\\\\" + model_name\n",
    "        \n",
    "        mu_paths.append(mu_path)\n",
    "\n",
    "        generate_script(script_path,mu_path)\n",
    "        generate_mupp(model_original,mu_path)\n",
    "\n",
    "        os.remove(mu_path) if os.path.exists(mu_path) else None\n",
    "        shutil.copyfile(model_original, mu_path)\n",
    "\n",
    "        sql = \"SELECT MUID FROM msm_Project WHERE enable_hd = 1\"\n",
    "        muids = list(sql_to_df(sql,mu_path).muid)\n",
    "        for muid in muids:\n",
    "            muid_new = muid.replace(str(year_original) + 'p',str(year) + 'p')\n",
    "            sql = \"UPDATE msm_Project SET scenarioname = '\" + scenario + \"', muid = '\" + muid_new + \"' WHERE muid = '\" + muid + \"'\"\n",
    "            execute_sql(sql, mu_path)\n",
    "\n",
    "            sql = \"UPDATE msm_ProjectOutput SET simulationid = '\" + muid_new + \"' WHERE simulationid = '\" + muid + \"'\"\n",
    "            execute_sql(sql, mu_path)\n",
    "\n",
    "        sql = \"UPDATE msm_Project SET IncludeToBatchNo = 0 WHERE MUID LIKE '%h-AES_%'\"\n",
    "        execute_sql(sql, mu_path)\n",
    "\n",
    "        for turnon in turnons:\n",
    "            sql = \"UPDATE msm_Project SET IncludeToBatchNo = 1 WHERE MUID LIKE '%h-AES_%' AND MUID LIKE '%\" + turnon + \"%'\"\n",
    "            execute_sql(sql, mu_path)\n",
    "\n",
    "        pop_df_melt_year = pop_df_melt[pop_df_melt.Year==year]\n",
    "\n",
    "        df_to_sql(pop_df_melt_year,'New_Population',mu_path)\n",
    "\n",
    "        sql = \"UPDATE msm_Loadpoint SET Population = \"\n",
    "        sql += \"(SELECT Value FROM New_Population WHERE MUID = msm_Loadpoint.muid AND Major_Type = 'Pop')\"\n",
    "        execute_sql(sql, mu_path)\n",
    "\n",
    "        sql = \"UPDATE msm_Loadpoint SET ICIArea = \"\n",
    "        sql += \"(SELECT Value FROM New_Population WHERE MUID = msm_Loadpoint.muid AND Major_Type = 'Area')\"\n",
    "        execute_sql(sql, mu_path)\n",
    "\n",
    "        sql = \"UPDATE msm_Loadpoint SET loadflow = PerCapitaLoad * Population / 86400 WHERE LoadCategory = 'Mixed' OR LoadCategory = 'ResLD' OR LoadCategory = 'ResHD'\"\n",
    "        execute_sql(sql, mu_path)\n",
    "        sql = \"UPDATE msm_Loadpoint SET loadflow = PerAreaLoad * ICIArea / 86400 WHERE LoadCategory = 'Commercial' OR LoadCategory = 'Industrial' OR LoadCategory = 'Institutional'\"\n",
    "        execute_sql(sql, mu_path)\n",
    "\n",
    "        sql = \"DROP TABLE New_Population\"\n",
    "        execute_sql(sql, mu_path)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5293ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##AFTER UPDATE AND SAVE YOU MUST RESTART THE KERNEL IN JUPYTER NOTEBOOK TO UPDATE VARIABLES!\n",
    "\n",
    "##Remember to insert r in front of all paths, e.g. r\"J:\\SEWER_AREA_MODELS\\FSA\\03_SIMULATION_WORK\\Calibration_2022\\MODEL\"\n",
    "\n",
    "\n",
    "#FSA System Assessment\n",
    "\n",
    "generate_future = False\n",
    "generate_sealed_vfd = True\n",
    "generate_xadwf = False\n",
    "generate_bsf = False\n",
    "\n",
    "output_folder = r'J:\\SEWER_AREA_MODELS\\FSA\\03_SIMULATION_WORK\\System_Assessment\\Model_Generation'\n",
    "\n",
    "#For future models\n",
    "model_original = r\"J:\\SEWER_AREA_MODELS\\FSA\\01_MASTER_MODEL\\MODEL\\FSA_Base_2021pop.sqlite\"\n",
    "year_original = 2021\n",
    "version = 103\n",
    "year_scenario_list = []\n",
    "year_scenario_list.append([2030,'2030_Network',['Ex']])\n",
    "year_scenario_list.append([2040,'2030_Network',['2050M','2050H']])\n",
    "year_scenario_list.append([2050,'2030_Network',['2050H']])\n",
    "year_scenario_list.append([2060,'2030_Network',['2050H','2100H']])\n",
    "population_sheet = r\"J:\\SEWER_AREA_MODELS\\FSA\\02_MODEL_COMPONENTS\\04_DATA\\01. POPULATION\\FSA_Master_Population_File_Update14a15a.xlsx\"\n",
    "population_tab = 'MPF Update 14a'\n",
    "\n",
    "#For sealed VFD\n",
    "vfd_all = True\n",
    "seal_all = True\n",
    "\n",
    "excluded_asset_names = []\n",
    "excluded_asset_names.append('Golden Ears SSO Tank')\n",
    "excluded_asset_names.append('NW CSO Tank Cleanout Pumps')\n",
    "excluded_asset_names.append('')\n",
    "\n",
    "weir_turn_offs = []\n",
    "weir_turn_offs.append('NS4-SSO')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2eca1f7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seal_all and vfd_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ba8f3cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3499044034.py, line 59)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\hloecke\\AppData\\Local\\Temp\\ipykernel_24068\\3499044034.py\"\u001b[1;36m, line \u001b[1;32m59\u001b[0m\n\u001b[1;33m    sql = \"SELECT assetname, muid, startlevel from msm_Pump ORDER BY assetname, startlevel\"\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "if generate_sealed_vfd:\n",
    "\n",
    "    if not generate_future:\n",
    "        mu_paths = []\n",
    "        for f in os.listdir(output_folder):\n",
    "            if f[-7:]==\".sqlite\" and not 'VFD' in f:\n",
    "                mu_paths.append(output_folder + '\\\\' + f)\n",
    " \n",
    "    if seal_all and vfd_all:\n",
    "        suffix = \"S_V_\"\n",
    "        file_suffix = \"_Sealed_VFD\"\n",
    "    elif seal_all and not vfd_all:\n",
    "        suffix = \"S_\"\n",
    "        file_suffix = \"_Sealed\"\n",
    "    elif not seal_all and vfd_all:        \n",
    "        suffix = \"V_\"\n",
    "        file_suffix = \"_VFD\" \n",
    "    else:\n",
    "        message = \"Error! generate_sealed_vfd set to True but both vfd_all and seal_all set to False\"\n",
    "        raise ValueError(message)\n",
    "\n",
    "    for mu_path_original in mu_paths[2:]:\n",
    "        \n",
    "        \n",
    "\n",
    "        sqls = []\n",
    "\n",
    "        mu_path = mu_path_original[:-7] + file_suffix + \".sqlite\"\n",
    "        os.remove(mu_path) if os.path.exists(mu_path) else None\n",
    "        shutil.copyfile(mu_path_original, mu_path)\n",
    "        \n",
    "        generate_script(script_path,mu_path)\n",
    "        generate_mupp(mu_path_original,mu_path)\n",
    "        \n",
    "        print('Generating ' + mu_path)\n",
    "\n",
    "        if vfd_all == True:\n",
    "\n",
    "            pumps_for_VFD = []\n",
    "            pumps_turn_off = []\n",
    "\n",
    "            sqls.append(\"INSERT INTO ms_Tab (muid,altid,active,description,typeno) SELECT 'Generic_Pump_Min',0,1,'Use for PS passing all inflow',2\")\n",
    "            sqls.append(\"INSERT INTO ms_Tab (muid,altid,active,description,typeno) SELECT 'Generic_Pump_Max',0,1,'Use for PS passing all inflow',2\")\n",
    "\n",
    "            sqls.append(\"INSERT INTO ms_TabD (muid,altid,active,tabid,sqn,value1,value2) SELECT 'Generic_Pump_Max-1',0,1,'Generic_Pump_Max',1, 0, 50\")\n",
    "            sqls.append(\"INSERT INTO ms_TabD (muid,altid,active,tabid,sqn,value1,value2) SELECT 'Generic_Pump_Max-2',0,1,'Generic_Pump_Max',2, 100, 50\")\n",
    "            \n",
    "            sqls.append(\"INSERT INTO ms_TabD (muid,altid,active,tabid,sqn,value1,value2) SELECT 'Generic_Pump_Min-1',0,1,'Generic_Pump_Min',1, 0, 0\")\n",
    "            sqls.append(\"INSERT INTO ms_TabD (muid,altid,active,tabid,sqn,value1,value2) SELECT 'Generic_Pump_Min-2',0,1,'Generic_Pump_Min',2, 100, 0\")\n",
    "\n",
    "            \n",
    "            sql = \"SELECT MUID FROM msm_Project WHERE enable_hd = 1\"\n",
    "            muids = list(sql_to_df(sql,mu_path).muid)\n",
    "            for muid in muids:\n",
    "                if len(muid) <= suffix:\n",
    "                    muid_new = muid & suffix\n",
    "                    sqls.append(\"UPDATE msm_Project SET muid = '\" + muid_new + \"'' WHERE muid = '\" + muid + \"'\")        \n",
    "                    sqls.append(\"UPDATE msm_ProjectOutput SET simulationid = '\" + muid_new + \"' WHERE simulationid = '\" + muid + \"'\"\n",
    "            sql = \"SELECT assetname, muid, startlevel from msm_Pump ORDER BY assetname, startlevel\"\n",
    "            pumps = sql_to_df(sql,mu_path)\n",
    "\n",
    "            previous_asset = 'xxxx'\n",
    "            for index, row in pumps.iterrows():\n",
    "                asset = str(row[0])\n",
    "                if not asset in excluded_asset_names and asset != 'None':\n",
    "                    muid = str(row[1])\n",
    "                    if previous_asset != asset:\n",
    "                        pumps_for_VFD.append(muid)\n",
    "                    else:\n",
    "                        pumps_turn_off.append (muid)\n",
    "                    previous_asset = asset\n",
    "                else:\n",
    "                    print ('Skipped ' + str(row[1]))\n",
    "\n",
    "            for pump_for_VFD in pumps_for_VFD:\n",
    "                sqls.append(\"UPDATE msm_Pump SET qmaxsetid = 'Generic_Pump_Max',  qminsetid = 'Generic_Pump_Min', speedno = 2, captypeno = 2, controltypeno = 1, startlevel = stoplevel + 0.1,  wetwellsetpoint = stoplevel + 0.1  WHERE muid = '\" + pump_for_VFD + \"'\")\n",
    "                sqls.append(\"UPDATE msm_RTC SET applyno = 0 WHERE pumpid = '\" + pump_for_VFD + \"'\")\n",
    "\n",
    "            for pump_turn_off in pumps_turn_off:\n",
    "                sqls.append(\"UPDATE msm_Pump SET startlevel = startlevel + 100, stoplevel = stoplevel + 100,  controltypeno = 1 WHERE MUID = '\" + pump_turn_off + \"'\")\n",
    "                sqls.append(\"UPDATE msm_RTC SET applyno = 0 WHERE pumpid = '\" + pump_turn_off + \"'\")\n",
    "\n",
    "\n",
    "        execute_sql(sqls, mu_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39_mike",
   "language": "python",
   "name": "py39_mike"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
