{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59c43835",
   "metadata": {},
   "source": [
    "## TOOL UPDATED: June 20 2023, Henrik Loecke\n",
    "\n",
    "##Double click this cell to see full description.\n",
    "\n",
    "##You must restart the kernel after updating System_Assessment_Variables!\n",
    "\n",
    "<!-- \n",
    "\n",
    "To run this notebook, click menu Cell -> Run All\n",
    "\n",
    "User input has been moved away from this notebook so it can easily be replaced by new versions.\n",
    " \n",
    "Please open System_Assessment_Variables, in the same folder as this notebook, to edit user input there.\n",
    "\n",
    "All variables with path must start with 'r', e.g. r'C:\\Projects'\n",
    "\n",
    "It must contain the following variables:\n",
    "\n",
    "model_area:                          Short area name like 'VSA' or LISA'  \n",
    "result_specs_csv:                    CSV file linking network and runoff result file. Only needed if runoff imported.\n",
    "ps_specs_csv:                        CSV file containing PS firm and station capacities.\n",
    "summation_csv:                       Summarizes which elements are to be summed up, mostly used for outfalls.\n",
    "summation_ps_csv:                    Summarizes which elements are to be summed up for PS assessment.\n",
    "outfall_csv:                         Summarizes all outfalls that are to have overflows displayed on the map.    \n",
    "result_folder:                       Folder path of result files.\n",
    "map_folder:                          Folder path where maps are generated and where this tool returns output.\n",
    "set_pipe_volume_0_when_max_pipe_flow_less_than_cms: \n",
    "                                     If the maximum absolute (allow reverse flow) is below this, set to 0.\n",
    " -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42623f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PERMANENT CELL 1\n",
    "\n",
    "import os\n",
    "import re\n",
    "import mikeio\n",
    "import mikeio1d\n",
    "from mikeio1d.res1d import Res1D\n",
    "from mikeio.dfs0 import Dfs0\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import traceback\n",
    "import ctypes\n",
    "MessageBox = ctypes.windll.user32.MessageBoxA\n",
    "from System_Assessment_Variables import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ba294ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PERMANENT CELL 2\n",
    "#List all network elements to be imported.\n",
    "\n",
    "try:\n",
    "\n",
    "    flow_pipes = set()\n",
    "\n",
    "    #Add outfalls\n",
    "    outfalls = pd.read_csv(outfall_csv,dtype={'Weir': str,'Outfall': str})\n",
    "    outfalls['Res_ID'] = ''\n",
    "    for index, row in outfalls.iterrows():\n",
    "        prefix = ''\n",
    "\n",
    "        if row['Layer'] == 'Summation':\n",
    "            prefix = ''\n",
    "        elif row['Layer'] != 'msm_Link':\n",
    "            prefix = row['Layer'][4:] + ':'\n",
    "        muid = prefix + row['Weir']\n",
    "\n",
    "        if row['Layer'] != 'Summation':\n",
    "            outfalls.iloc[index,3] = row['Layer'][4:]\n",
    "        outfalls.iloc[index,4] = muid\n",
    "\n",
    "\n",
    "        if row['Layer'] != 'Summation':\n",
    "            flow_pipes.add(muid)\n",
    "\n",
    "    catchments = set()\n",
    "    summation_df = pd.read_csv(summation_csv,dtype={'MUID': str,'SUMTO': str})\n",
    "    summation_df_ps = pd.read_csv(summation_ps_csv,dtype={'MUID': str,'SUMTO': str})\n",
    "    summation_df = pd.concat([summation_df,summation_df_ps])\n",
    "    summation_df.reset_index(drop=True,inplace=True)\n",
    "    summation_df['TS_ID'] = ''\n",
    "\n",
    "    for index, row in summation_df.iterrows():\n",
    "        prefix = ''\n",
    "\n",
    "        if row['Layer'] != 'msm_Link' and row['Layer'] != 'ms_Catchment':\n",
    "            prefix = row['Layer'][row['Layer'].find('_') + 1:] + ':'\n",
    "\n",
    "        muid = prefix + row['MUID']\n",
    "        summation_df.iloc[index,3] = muid\n",
    "\n",
    "        summation_df.iloc[index,0] = muid\n",
    "\n",
    "        if '-Negative' in muid:\n",
    "            muid = muid[:-9]\n",
    "\n",
    "        if row['Layer'] == 'ms_Catchment':\n",
    "            catchments.add(muid)\n",
    "        else:\n",
    "            flow_pipes.add(muid)\n",
    "        \n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 2', b'Error', 0)\n",
    "    raise ValueError(\"Error\")\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b189276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing network FSA_GA_EX-10y-24h-AES_2018p_DSS12Default_Network_HD.res1d at 2023-09-01 14:17:56.374969\n",
      "Importing network FSA_GA_EX-10y-24h-AES_2021p_Base.res1d at 2023-09-01 14:19:04.597245\n",
      "Importing spilling FSA_GA_EX-10y-24h-AES_2021p_Base.ADDOUT.res1d at 2023-09-01 14:20:08.484444\n",
      "Importing network FSA_GA_EX-10y-24h-AES_2021p_DSS14Default_Network_HD_M.res1d at 2023-09-01 14:20:26.381343\n",
      "Importing network FSA_WWF_2021-11-12_5d_2021pop_BaseDefault_Network_HD.res1d at 2023-09-01 14:22:07.219746\n",
      "Doing summation at 2023-09-01 14:29:15.283165\n",
      "20th Street PS\n",
      "Baynes PS\n",
      "Cloverdale PS\n",
      "Crescent Beach PS\n",
      "East Richmond PS\n",
      "Katzie PS\n",
      "Langley PS\n",
      "Marshend PS\n",
      "New Sapperton PS\n",
      "Port Coquitlam PS\n",
      "Port Moody PS\n",
      "Queensborough PS\n",
      "Royal Avenue PS\n",
      "Short Street PS\n",
      "Sperling PS\n",
      "Westridge PS #1\n",
      "Westridge PS #2\n",
      "White Rock PS\n",
      "20th Street PS\n",
      "Baynes PS\n",
      "Cloverdale PS\n",
      "Crescent Beach PS\n",
      "East Richmond PS\n",
      "Katzie PS\n",
      "Langley PS\n",
      "Marshend PS\n",
      "New Sapperton PS\n",
      "Port Coquitlam PS\n",
      "Port Moody PS\n",
      "Queensborough PS\n",
      "Royal Avenue PS\n",
      "Short Street PS\n",
      "Sperling PS\n",
      "Westridge PS #1\n",
      "Westridge PS #2\n",
      "White Rock PS\n",
      "20th Street PS\n",
      "Baynes PS\n",
      "Cloverdale PS\n",
      "Crescent Beach PS\n",
      "East Richmond PS\n",
      "Katzie PS\n",
      "Langley PS\n",
      "Marshend PS\n",
      "New Sapperton PS\n",
      "Port Coquitlam PS\n",
      "Port Moody PS\n",
      "Queensborough PS\n",
      "Royal Avenue PS\n",
      "Short Street PS\n",
      "Sperling PS\n",
      "Westridge PS #1\n",
      "Westridge PS #2\n",
      "White Rock PS\n",
      "20th Street PS\n",
      "Baynes PS\n",
      "Cloverdale PS\n",
      "Crescent Beach PS\n",
      "East Richmond PS\n",
      "Katzie PS\n",
      "Langley PS\n",
      "Marshend PS\n",
      "New Sapperton PS\n",
      "Port Coquitlam PS\n",
      "Port Moody PS\n",
      "Queensborough PS\n",
      "Royal Avenue PS\n",
      "Short Street PS\n",
      "Sperling PS\n",
      "Westridge PS #1\n",
      "Westridge PS #2\n",
      "White Rock PS\n"
     ]
    }
   ],
   "source": [
    "#PERMANENT CELL 3\n",
    "#Import network (and runoff if applicable) result files\n",
    "\n",
    "pipes_set_to_zero = set()\n",
    "\n",
    "try:\n",
    "    ps_specs = pd.read_csv(ps_specs_csv)\n",
    "    ps_specs.set_index('PS',inplace=True)\n",
    "\n",
    "    first_runoff = True\n",
    "    if len(catchments) > 0:\n",
    "        result_specs = pd.read_csv(result_specs_csv)\n",
    "        for f in result_specs.Runoff.unique():\n",
    "\n",
    "                print('Importing ' + f + \" at \" + str(dt.datetime.now()))\n",
    "                res1d = Res1D(result_folder + '\\\\' + f)\n",
    "\n",
    "                for catchment in catchments:\n",
    "\n",
    "                    catchment_df = pd.DataFrame(index = res1d.time_index)\n",
    "                    catchment_df['ResultFile'] = f\n",
    "                    catchment_df['MUID'] = catchment\n",
    "                    catchment_df['DateTimeRef'] = catchment_df.index\n",
    "                    ts = res1d.query.GetCatchmentValues(catchment, \"TotalRunOff\")\n",
    "                    if ts == None:\n",
    "                        raise ValueError(\"Catchment '\" + catchment + \"' not found in \" + f)\n",
    "                    catchment_df['Discharge'] = ts\n",
    "                    catchment_df['Discharge'] = catchment_df['Discharge']*1000\n",
    "\n",
    "                    catchment_df['Seconds'] = catchment_df.index.to_series().diff().astype('timedelta64[s]').fillna(method='bfill')\n",
    "                    catchment_df['Volume'] = catchment_df.Discharge * catchment_df.Seconds / 1000\n",
    "                    catchment_df.drop(columns=['Seconds'],inplace=True)\n",
    "\n",
    "\n",
    "                    if first_runoff == True:\n",
    "                        catchment_df_all = catchment_df.copy()\n",
    "                    else:\n",
    "                        catchment_df_all = pd.concat([catchment_df_all,catchment_df])\n",
    "                    first_runoff = False\n",
    "\n",
    "        result_specs.rename(columns={'Runoff':'ResultFile'},inplace=True)\n",
    "        catchment_df_all = pd.merge(catchment_df_all,result_specs,how='left',on=['ResultFile'])\n",
    "        catchment_df_all.drop(columns='ResultFile',inplace=True)\n",
    "        catchment_df_all.rename(columns={'Network':'ResultFile'},inplace=True)\n",
    "        catchment_df_all = catchment_df_all[['ResultFile', 'MUID', 'DateTimeRef', 'Discharge', 'Volume']]\n",
    "\n",
    "    results = []\n",
    "    first_level = True\n",
    "    first_flow = True\n",
    "    first_velocity = True\n",
    "\n",
    "    res_stats = []\n",
    "    for i in range(20):\n",
    "        res_stats.append(['FORCETEXT','FORCETEXT','FORCETEXT',0])\n",
    "\n",
    "    network_result_files = []\n",
    "    \n",
    "    if len(catchments) > 0:    \n",
    "        network_result_files = list(result_specs.Network.unique())\n",
    "    else:   \n",
    "        for f in os.listdir(result_folder):\n",
    "            if f[-6:]=='.res1d' and not 'ADDOUT' in f and not 'RR' in f and not 'UserSpecified' in f and not 'hotstart' in f.lower() and not 'runoff' in f.lower() and not 'rdii' in f.lower(): \n",
    "                network_result_files.append(f)\n",
    "    \n",
    "    for f in network_result_files:\n",
    "\n",
    "        res1d = Res1D(result_folder + '\\\\' + f)\n",
    "        timestep_seconds = (max(res1d.time_index) - min(res1d.time_index)).total_seconds() / (len(res1d.time_index)-1)\n",
    "        reaches = res1d.data.Reaches\n",
    "        nodes = res1d.data.Nodes\n",
    "\n",
    "        print (\"Importing network \" + f + \" at \" + str(dt.datetime.now()))\n",
    "\n",
    "        first_round = True\n",
    "\n",
    "        for i, reach in enumerate(reaches):\n",
    "\n",
    "            muid = reach.Id[:reach.Id.rfind('-')]\n",
    "\n",
    "            max_flow = max(res1d.query.GetReachEndValues(muid, 'Discharge'))\n",
    "            min_flow = min(res1d.query.GetReachEndValues(muid, 'Discharge'))\n",
    "            max_abs_flow = max(abs(max_flow),abs(min_flow))\n",
    "            if max_abs_flow < set_pipe_volume_0_when_max_pipe_flow_less_than_cms and \\\n",
    "                                                    not( muid.startswith('Orifice:') or \\\n",
    "                                                        muid.startswith('Pump:') or \\\n",
    "                                                        muid.startswith('Weir:') or muid.startswith('Valve:') ):\n",
    "                volume = 0\n",
    "            else:\n",
    "                volume = sum(res1d.query.GetReachEndValues(muid, 'Discharge')) * timestep_seconds\n",
    "\n",
    "            res_stats.append([f,muid,'MAXFLOW',max_flow])\n",
    "            res_stats.append([f,muid,'VOLUME',volume])\n",
    "            try:\n",
    "                velocity = max(res1d.query.GetReachEndValues(muid, \"FlowVelocity\"))\n",
    "                res_stats.append([f,muid,'MAXVELOCITY',velocity])\n",
    "            except:\n",
    "                pass #only pipes have velocity\n",
    "\n",
    "            if muid in flow_pipes or muid in list(summation_df.MUID):\n",
    "\n",
    "                values = res1d.query.GetReachEndValues(muid, 'Discharge')\n",
    "                flow_df = pd.DataFrame(index = res1d.time_index)\n",
    "                flow_df['ResultFile'] = f\n",
    "                flow_df['MUID'] = muid\n",
    "                flow_df['DateTimeRef'] = flow_df.index\n",
    "                if max_abs_flow < set_pipe_volume_0_when_max_pipe_flow_less_than_cms and \\\n",
    "                                                    not( muid.startswith('Orifice:') or \\\n",
    "                                                        muid.startswith('Pump:') or \\\n",
    "                                                        muid.startswith('Weir:') or muid.startswith('Valve:') ):\n",
    "                    flow_df['Discharge'] = 0\n",
    "                    pipes_set_to_zero.add(muid)\n",
    "                else:\n",
    "                    flow_df['Discharge'] = values  \n",
    "                    \n",
    "                flow_df['Discharge'] = flow_df['Discharge'] * 1000\n",
    "\n",
    "                flow_df['Seconds'] = flow_df.index.to_series().diff().astype('timedelta64[s]').fillna(method='bfill')\n",
    "                flow_df['Volume'] = flow_df.Discharge * flow_df.Seconds / 1000\n",
    "                flow_df.drop(columns=['Seconds'],inplace=True)\n",
    "\n",
    "                if first_flow == True:\n",
    "                    flow_df_all = flow_df.copy()\n",
    "                else:\n",
    "                    flow_df_all = pd.concat([flow_df_all,flow_df])\n",
    "                first_flow = False\n",
    "\n",
    "        first_round = False\n",
    "\n",
    "        for node in nodes:\n",
    "            muid = node.Id\n",
    "            max_level = max(res1d.query.GetNodeValues(muid, \"WaterLevel\"))\n",
    "            res_stats.append([f,muid,'MAXLEVEL',max_level])\n",
    "\n",
    "        flood_types = ['WaterFlowRateAboveGround', 'WaterSpillDischarge']\n",
    "        \n",
    "        flooding_result_available = False\n",
    "        if os.path.exists(result_folder + '\\\\' + f[:-6] + '.ADDOUT.res1d'):\n",
    "            print (\"Importing spilling \" + f[:-6] + \".ADDOUT.res1d at \" + str(dt.datetime.now()))\n",
    "            res1d = Res1D(result_folder + '\\\\' + f[:-6] + '.ADDOUT.res1d')\n",
    "            flooding_result_available = True\n",
    "        elif flood_types[0] in res1d.quantities or flood_types[1] in res1d.quantities:\n",
    "            pass #Maintain same res1d file which has flooding (MIKE+)    \n",
    "            flooding_result_available = True\n",
    "        else:\n",
    "            print('WARNING! ' + f[:-6] + '.ADDOUT.res1d does not exist and flooding not available in ' + f + '.')\n",
    "        \n",
    "        if flooding_result_available == True:         \n",
    "            \n",
    "            df = pd.DataFrame(index=res1d.time_index)\n",
    "\n",
    "            for node in res1d.data.Nodes:\n",
    "                muid = node.Id\n",
    "                for i, flood_type in enumerate(flood_types):\n",
    "                    flood_rate = res1d.query.GetNodeValues(muid,flood_type)\n",
    "                    if flood_rate != None:\n",
    "                        if max(flood_rate) > 0:\n",
    "                            res_stats.append([f,muid,'MAXSPILLRATE',max(flood_rate)])\n",
    "                            if i == 0: #Normal\n",
    "                                spill_volume = max(res1d.query.GetNodeValues(muid,'WaterVolumeAboveGround'))\n",
    "                            else:\n",
    "                                spill_volume =  sum(flood_rate) * timestep_seconds\n",
    "                            res_stats.append([f,muid,'SPILLVOLUME',spill_volume])\n",
    "\n",
    "    if len(catchments) > 0: \n",
    "        catchment_df_all_filter  = catchment_df_all.copy()\n",
    "        times = list(flow_df_all.DateTimeRef.unique())\n",
    "        catchment_df_all_filter.set_index(catchment_df_all_filter.DateTimeRef,inplace=True)\n",
    "        catchment_df_all_filter =  catchment_df_all_filter[catchment_df_all_filter['DateTimeRef'].isin(times)]\n",
    "\n",
    "    negatives = list(summation_df.query(\"MUID.str.endswith('-Negative')\").MUID.unique())\n",
    "    original_negatives = []\n",
    "    for negative in negatives:\n",
    "        original_negatives.append(negative[:-9])\n",
    "    negatives_df = flow_df_all[flow_df_all.MUID.isin(original_negatives)].copy()\n",
    "    negatives_df.MUID = negatives_df.MUID + '-Negative'\n",
    "    negatives_df.Discharge = negatives_df.Discharge * -1\n",
    "    negatives_df.Volume = negatives_df.Volume * -1\n",
    "\n",
    "    flow_df_all = pd.concat([flow_df_all,negatives_df])\n",
    "\n",
    "    if len(catchments) > 0: \n",
    "        flow_df_all = pd.concat([flow_df_all,catchment_df_all_filter])\n",
    "\n",
    "    #Summation\n",
    "    print (\"Doing summation at \" + str(dt.datetime.now()))             \n",
    "\n",
    "    df_result_sum = pd.merge(flow_df_all,summation_df,how='inner',on=['MUID'])\n",
    "    df_result_sum = df_result_sum.groupby(['ResultFile','SUMTO','DateTimeRef']).agg({'Discharge':'sum','Volume':'sum'})\n",
    "    df_result_sum.reset_index(inplace=True)\n",
    "    df_result_sum.set_index('DateTimeRef',drop=False,inplace=True)\n",
    "    df_result_sum.rename(columns = {'SUMTO':'MUID'},inplace=True)\n",
    "\n",
    "    for index, row in df_result_sum[['ResultFile','MUID','Discharge']].groupby(['ResultFile','MUID']).max().reset_index().iterrows():\n",
    "        res_stats.append([row['ResultFile'],row['MUID'],'MAXFLOW',row['Discharge']/1000])\n",
    "\n",
    "    for index, row in df_result_sum[['ResultFile','MUID','Volume']].groupby(['ResultFile','MUID']).sum().reset_index().iterrows():\n",
    "        res_stats.append([row['ResultFile'],row['MUID'],'VOLUME',row['Volume']])\n",
    "\n",
    "    flow_df_all = pd.concat([flow_df_all,df_result_sum])\n",
    "\n",
    "    not_founds = []\n",
    "    for muid in summation_df.TS_ID.unique():\n",
    "        if not muid in flow_df_all.MUID.unique():\n",
    "            not_founds.append(muid)\n",
    "\n",
    "    if len(not_founds) > 0:\n",
    "        error_message = 'The following elements in summation_df were not found: \\n' \n",
    "        error_message += ','.join(not_founds) + '.\\n'\n",
    "        raise ValueError(error_message)\n",
    "\n",
    "    res_stats_df = pd.DataFrame(res_stats,columns=['RESULT','MUID','TYPE','READING'])\n",
    "\n",
    "    res_stats_df.to_csv(map_folder + '\\\\Res_Stats.csv',index=False)\n",
    "\n",
    "    ps_stats = []\n",
    "\n",
    "    for result in network_result_files:\n",
    "        for ps in summation_df_ps.SUMTO.unique():\n",
    "            if ps[-7:] != 'Outflow':\n",
    "                print (ps)\n",
    "                inflow = max(flow_df_all.query(\"ResultFile == '\" + result + \"' & MUID == '\" + ps + \"'\").Discharge)\n",
    "                outflow = max(flow_df_all.query(\"ResultFile == '\" + result + \"' & MUID == '\" + ps + ' Outflow' + \"'\").Discharge)\n",
    "                firm_capacity = ps_specs.loc[ps,'Firm Capacity']\n",
    "                station_capacity = ps_specs.loc[ps,'Wet Weather Capacity']\n",
    "                if inflow >= station_capacity:\n",
    "                    color = \"BLACK\"\n",
    "                elif inflow >= firm_capacity:\n",
    "                    color = \"RED\"\n",
    "                else:\n",
    "                    color = \"GREEN\"\n",
    "                ps_stats.append([ps,result,inflow,outflow,firm_capacity,station_capacity,color])\n",
    "    ps_stats_df = pd.DataFrame(ps_stats,columns = ['PS','ResultFile','MaxTotalInflow','MaxTotalOutflow', 'FirmCapacity', 'StationCapacity', 'Color'])\n",
    "    ps_stats_df.to_csv(map_folder + '\\\\PS_Stats.csv',index=False)\n",
    "\n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 3', b'Error', 0)\n",
    "    raise ValueError(\"Error\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dd81a0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PERMANENT CELL 4\n",
    "MessageBox(None,b'All cells ran successfully.', b'Done', 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39_mike",
   "language": "python",
   "name": "py39_mike"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
